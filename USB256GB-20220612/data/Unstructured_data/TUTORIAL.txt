################################################################################
#
# 金沢大学 共通教育科目
# 「シェルスクリプトを用いた『大規模データ処理』演習」 2021 向け
# 「2021年度Q2 シェルスクリプトを用いた『ものグラミングと大規模データ処理』演習」向け
#
# 非定型データ（東京五輪ツイートデータ）用サンプルスクリプト
#
# 作成者・連絡先: USP研究所 松浦智之 <t-matsuura@usp-lab.com>
# 最終更新日    : 2021年6月9日
#
################################################################################



===== ０. はじめに =============================================================

このファイルには、上記演習のためのサンプルスクリプト（ワンライナーコマンド）と
その説明を収録しています。

なお、サンプルスクリプトでは"Personal Tukubai for Academic"（PT4A）を
用いていますので、予めインストールを済ませておいてください。PT4Aに関しては、
別途配布れされる定型データのUSBフラッシュメモリを参照してください。

また、一部のTukubaiコマンドのマニュアルページはwebブラウザーでも閲覧可能です。
（今後説明するコマンドは随時増やしていく予定）
https://pt.usp-lab.com/html/man/


===== １. ツイートデータ分析の（今回の）方針 ===================================

○様々な分析テーマについて

ツイート情報には文章や投稿者名、位置情報（全体の1%未満）が含まれており、
これらを分析すれば、

・どんな単語や固有名詞（人名・地名・組織名等）が多く含まれているか
・どの地域からツイートが多く発信されているか

ということもわかるでしょう。さらには、

・特手の人や組織に対して、好感意見が優勢か、嫌悪意見が優勢か。
・ある話題・人・物は、別のどの話題・人・物と関連が強いか。
・位置情報を発している人はどういう移動をしているか。

など、いくらでも興味深い分析テーマがあるでしょう。しかしながら、限られた
講義時間の中で、それらすべてを説明し、演習することはできません。そこで今回は
次のテーマを演習課題とします。

○ツイート頻度の時間推移を分析する

今回収集したツイートは、「オリンピック」等を含むという条件ですが、
例えばそれが、2020年3月24日21時台にたくさん発生していたとしたら、人々は
その日にオリンピック関連で何か盛り上がっていたと想像できます。
（実際には、その頃にオリンピックの1年延期が発表されていました）

そこで、そのような世間を揺るがすようなオリンピック関連の出来事がいつ
発生していたのか。そしてその時、具体的に何が原因でツイートがそんなに
盛んに投稿されていたのかを解明してみましょう。

○分析の大まかな流れ

では具体的にどうやればいいのでしょうか。手順をまとめます。

0) 日単位、時単位、分単位、秒単位など、頻度を数える粒度を決めます。

1) （時単位だとするなら）各1時間のツイート数を数えます。
   ----------
   2020年01月01日00時台のツイート数はn個
   2020年01月01日01時台のツイート数はn個
          :
          :
   ----------

2) Microsoft Excel（有料）やLibre Office（無料）、Google スプレッドシート等の
   表計算ソフトに各時刻とその間のツイート数を与え、グラフを作図します。
   ※ 日単位、時単位、分単位、秒単位のどれでグラフを作図するかによって、
      傾向の見えやすさが変わりますし、あるいは別の傾向が見える場合があります。
      どれを選ぶかは試行錯誤しながら決めてください。

3) グラフを観察すると、極大になっている箇所が見えると思いますが、その中で
   気になる時刻（時単位だったら何年何月何日何時まで）をいくつかメモします。

4) （例えば2020年3月24日21時だったとして）、"20200324"というディレクトリーの
   中の"21.txt"というファイルを開いて閲覧してください。
   そこにある本文を眺めると、当時何が話題だったのかがわかります。


===== ２. 分析のためのコマンド入力の詳細 =======================================

分析に関する大まかな流れがわかったところで、実際のコマンド操作を解説します。
なお、ここで示すコマンドは飽くまで例ですから、必要に応じてアレンジしてください。

また、ツイートデータの構造に関しては、README.txtファイルに掲載していますので、
そちらを併読しながら読んでください。

○ツイート数の数え方（その１…時単位）

このデータは、1ツイートが1行に配置されています。従って、ツイート数を数える
という分析は、行数を数えるという操作に置き換えられます。そこで用いるのは
Tukubaiに収録されている"gyo"コマンドです。

$ gyo FILENAME

と書けば、"FILENAME"というファイルの行数がわかります。行数を調べたいファイルが
大量にある場合は一つ一つやるのは大変ですので、次のように書きます。

$ gyo -f FILENAME1 FILENAME2 FILENAME3 ...

上記の"-f"オプションは、ファイル名を併記させるためのものです。これがなければ、
どのファイルがどの行数なのか対応がわかりませんので付けるようにしましょう。

また、ワイルドカードの仕組みを使えば、例えば拡張子が".txt"のものを一気に
指定できます。

$ gyo -f *.txx

しかし、日を跨ぐなどで、別ディレクトリーにあるファイルも一気に数えたい場合には
次のように、"find"、"xargs"コマンドと併用してこなします。例えば今、本教材の
データディレクトリーの最上位（"Tokyo2020"）に居て、そのディレクトリーを含む
配下のディレクトリーすべてに収録されている"*.txt"ファイル全ての行数を数えたい
のであればこうします。

$ find . -name '*.txt' | xargs gyo -f
（全部表示するのは時間がかかるので、[CTRL]+[C]を押して中断しましょう）

"find"コマンドでは、指定したディレクトリー（上記例では"."）以下にあって、
なおかつ"-name"というオプションで指定した条件に一致するファイル名を全て
列挙しています。それを"xargs"コマンドに送ると、"xargs"コマンドは、その後ろに
指定されている文字列のさらに後ろに、流れて来たファイル名を付け足して実行します。
すなわち、

$ gyo -f ./20200101/00.txt ./20200101/01.txt ./20200101/02.txt ...

というコマンドを打ち込むのと同等の動作をします。

これで、2020/01/01 0時台から、2021/06/05 8時台までの全てが数えられます。
上記のコマンド（"find"で始まるもの）を実行するとこんな感じになるはずです。
----------
./20200101/00.txt 123  （数字は適当です!）
./20200101/01.txt 45
      :
----------
すなわち、"./年月日/時.txt ツイート数"のような感じです。

ただ、余計な文字が入っていて邪魔なので消してしまいましょう。不要な文字を
掃除するにはTukubaiの"fsed"というコマンドが便利です。これは、"sed"コマンドの
文字列置換を列単位に実行できるようにしたものです。次のように打ち込んで
実行してください。

$ find . -name '*.txt' | xargs gyo -f | fsed -e 's/[^0-9]//1'
（これも、全部表示するのは時間がかかるので、[CTRL]+[C]を押して中断しましょう）

上記の例では正規表現を使っていますので、それが分からない方は調べておいて
もらいたいのですが、"fsed"特有のルールとして、"s///"の後ろに数字を書く
というものがあります。これは、その数字が指し示す番号の列に対して置換を適用する
という意味です。また、（単純一致による置換ではなく）正規表現に基づく置換を
行うという意図を示すため、必ず"-e"オプションをつけてください。


さて、上記のコマンドを実行すると、結果はこんな感じになります。
----------
2020010100 123
2020010101 45
      :
----------
1列目の文字列が、年月日時を表す10桁の数字になりました。

今は画面に表示して終わりでしたが、後で利用できるようにするため、ファイルに
保存しましょう。そのために、次のように打ち込んでください。

$ find . -name '*.txt' | xargs gyo -f | fsed -e 's/[^0-9]//1' >Tokyo2020tw_hour-n.txt

なお、大量のデータを数えるので、終わるまで相当時間がかかります。
寝る前にコマンドを実行して、朝を待つのがいいかもしれません。

○ツイート数の数え方（その２…日単位）

その１では時単位でしたが、日単位にするのはどうすればいいでしょうか？
既に、日より細かい時の単位で数えてありますので、各日の0時台から23時台の数を
集計すればよいのですが、そこで使えるコマンドがTukubaiにはあります。
"sm2"というコマンドで、"sum-up"（サム・アップ）という集計操作がその名の
由来です。

既に"Tokyo2020tw_hour-n.txt"を作成済であるものとして例を示します。まずは、
次のように打ち込んで実行してください。

$ cat Tokyo2020tw_hour-n.txt | self 1.1.8 2
（今は最後まで表示しても仕方無いので、途中で [CTRL]+[C]を押して止めましょう）

すると、1列目が年月日の8桁だけになって、次のような表示になったはずです。
----------
20200101 123
20200101 45
      :
----------

"self"もまたTukubaiのコマンドの一つで、"SELect Fields"が由来です。
上の例の"self 1.1.8 2"とは、「1列目の1文字目から8文字と、2列目全てを選べ」
という意味であり、結果として、1列目にあった、年月日以外の不要な文字列が
削ぎ落されます。

そして次に、上記のコマンドに、次のようにして"sm2"コマンドを追加して
実行してください。

$ cat Tokyo2020tw_hour-n.txt | self 1.1.8 2 | sm2 1 1 2 2

すると、それまで、1列目に同じ年月日の行が複数（0時台から23時台までの24行）
ありましたが、一つの年月日は一行だけになったはずです。これは、同じ年月日
を持つ行にある2列目の値（行数）が合算・集約されたからです。

"sm2 1 1 2 2"とは、「1列目から1列目までが同じ列同士のものについて、
2行目から2行目までの各列をそれぞれ、縦（つまり行）に加算せよ」という意味です。
今回の例では、開始列と終了列がたまたま同じだったので、"1 1"とか"2 2"というのが
不自然に思えますが、例えば次のようなデータの集計には便利です。
----------
#店番  店名 年・月  食品売上高 雑貨売上高 家電売上高
0001   A店  2020001   11111111   21111111  31111111
0001   A店  2020002   11111112   21111112  31111112
0001   A店     :          :          :         :
0001   A店  2020012   11111122   21111122  31111122
0002   B店  2020001   11111111   21111111  31111111
0002   B店  2020002   11111112   21111112  31111112
0002   B店     :          :          :         :
0002   B店  2020012   11111122   21111122  31111122
0003   C店     :          :          :         :
  :     :      :          :          :         :
----------
↓ "sum 1 2 3 5" によって、同一の店番・店名（1,2列目）の
   各商品部門（3から5列目）毎の合計を求める
----------
#店番  店名 食品売上高 雑貨売上高 家電売上高
0001   A店   111111111  211111111  311111111
0002   B店   111111122  211111122  311111122
0003   C店     :          :            :
  :     :      :          :            :
----------

ツイート数集計の話に戻しますが、この結果もまた、後で利用できるように
次のようにしてファイルに書き込んでおきましょう。

$ cat Tokyo2020tw_hour-n.txt | self 1.1.8 2 | sm2 1 1 2 2 >Tokyo2020tw_day-n.txt

"Tokyo2020tw_day-n.txt"の中身は年月日の8桁が重複しない形になったはずです。
----------
20200101 12345
20200102 6789
      :
----------

○ツイート数の数え方（その３…分単位・秒単位）

各ファイルは時単位に区切って作られていますので、単純に各ファイルの行数を数えても
分単位や秒単位など、より細かい粒度でのツイート数はわかりません。
一から考えましょう。

README.txtでも示したように、ツイートデータの各ファイル内の行頭が
"YYYY/MM/DD-HH:MM:SS" （年/月/日-時:分:秒）
になっていることに注目してください。つまり、
秒単位であれば、これらの数字が完全に一致するものを数えればよいですし、
分単位であれば、これらの数字が分単位まで完全に一致するものを数えればよいのです。
というわけで、まずはそれぞれの場合において、不要な文字列をカット、すなわち
必要な文字列だけを抽出することを考えます。

そのためには「その２」で説明した"self"コマンドが使えます。
まずは、数える対象の全ファイルを開きます。これは、「その１」の最後で説明した
find、xargs、それに、単純にファイルの中身を表示する"cat"コマンドの応用です。

$ find . -name '*.txt' | xargs cat
（これも、全部表示するのは時間がかかるので、[CTRL]+[C]を押して中断しましょう）

上記を実行すると、次のような表示されたはずです。
----------
2020/01/01-00:00:00 （以降ツイートデータ）
2020/01/01-00:00:00 （以降ツイートデータ）
    :
2020/01/01-00:00:01 （以降ツイートデータ）
2020/01/01-00:00:01 （以降ツイートデータ）
    :
----------
そして今欲しいのは、時刻を表す1列目の、全て（秒単位の場合）または分までです。

そこで、"self"コマンドを次のように使います。

$ find . -name '*.txt' | xargs cat | self 1      ←秒単位の場合

$ find . -name '*.txt' | xargs cat | self 1.1.16 ←分単位の場合

秒単位の場合は1列目全てなので、単純に"self 1"でいいですが、
分単位の場合は1列目の1文字目から16文字だけが欲しいので、"self 1.1.16"です。

次は、今のようにして抽出した、年月日時分秒または年月日時分の文字列のうち、
同一のものの個数を数えます、これで分単位・秒単位のツイート数がわかります。
それにはTukubaiの"count"というコマンドを使います。
分単位、秒単位、どちらの場合も、後ろに" | count 1 1"を書き足してから
再実行してみたください。

$ find . -name '*.txt' | xargs cat | self 1 | count 1 1      ←秒単位

$ find . -name '*.txt' | xargs cat | self 1.1.16 | count 1 1 ←分単位

すると次のように表示されるはずです。
----------
2020/01/01-00:00:00 123
2020/01/01-00:00:01 45
        :
----------
または
----------
2020/01/01-00:00 123
2020/01/01-00:00 45
        :
----------
（これも、全部表示するのは時間がかかるので、[CTRL]+[C]を押して中断しましょう）

左側の文字列が時刻、右側の数字がその時刻に集まったツイート数です。

"count 1 1"というのは、「1列目から1列目までの文字列が一致する行の行数を数えよ」
という意味です。今回の例もまた、開始列と終了列が同一なので不自然に見えますが、
複数の列に渡って同一な行数を数えたい場合に役に立ちます。

さて、今数えた結果についても、後で利用できるように、1列目にある数字以外の文字の
掃除（fsedコマンドの使用）をして、さらにファイルに保存しましょう。

$ find . -name '*.txt' | xargs cat | self 1 | count 1 1 | fsed -e 's/[^0-9]//1' >Tokyo2020tw_sec-n.txt

$ find . -name '*.txt' | xargs cat | self 1.1.16 | count 1 1 | fsed -e 's/[^0-9]//1' >Tokyo2020tw_min-n.txt

○数えたファイルを細切れにする

日単位でツイート数を数えた場合ならファイルの行数はたかだか数百行です。
一方、秒単位で数えた場合、365日×12か月×30日×24時間×60分×60秒
くらいのオーダーの行数ですから、そのまま表計算ソフトに渡そうものなら、
表計算ソフトはフリーズするか強制終了するでしょう。そもそも
そのような大量の行のデータでグラフを描いてもまともに読めません。

もし秒単位にツイート数を数えたデータからグラフを書くのであれば、現実的には
どこか1日（0時から24時まで）のグラフにすることになるでしょう。
そこで、数え上げたデータの中から、特定の期間を抽出する方法を解説します。

秒単位に数えたファイル"Tokyo2020tw_sec-n.txt"が既にあり、ここから2020年7月24日の
秒毎のツイート数だけを抽出したいとしましょう。その場合は次のようなコマンドで
できます。

$ uawk '$1>=20200724000000 && $1<20200725000000' Tokyo2020tw_sec-n.txt
（結果が出るまで時間がかかるかもしれません）

"uawk"とはAWKコマンドの無駄を省いて高速化したTukubaiのコマンドです。
AWKコマンドの使い方を知っていれば説明不要かもしれませんが、一応説明します。
"uawk"の引数にある"$1>=20200724000000"とは、1列目の値が20200724000000以上
（つまり2020年7月24日0時0分0秒以上）であるかどうかの条件判定文です、同様にして
"$1<20200725000000"とは、1列目の値が20200725000000未満であるかどうかの
条件判定文であり、この両者を"&&"で結んでいるので両者のAND条件になります。
このようにして、時刻の範囲を14桁の数値で大小比較すれば簡単に範囲を絞り込めます。

もちろんこれも後で再利用できるよう、次のようにしてファイルに保存ししましょう。
$ uawk '$1>=20200724000000 && $1<20200725000000' Tokyo2020tw_sec-n.txt >Tokyo2020tw_sec-n_20200724.txt

○数えたツイート数を表計算ソフトに渡せる形式（CSV）にする

ここまで説明してきたようにして、数えたり、細切れにしてきたデータですが、
それらを表計算ソフトに読み込ませるには一工夫要ります。表計算ソフト側が賢ければ、
そちら側で何とかすることもできますが、UNIX（Tukubai）側でCSVファイル化する
という手段の方がおそらく簡単です。というわけで最後に、CSVファイル化の方法を
解説します。

その前にまず、時刻のフォーマットを直さねばなりません。現状では、

・日単位の場合の1列目 ... "20200724"等の8桁整数
・時単位の場合の1列目 ... "2020072421"等の10桁整数
・分単位の場合の1列目 ... "202007242100"等の12桁整数
・秒単位の場合の1列目 ... "20200724210010"等の14桁整数

のように、いずれも単なる整数であるため、表計算ソフトからは時刻として
認識されません。表計算ソフトで認識されるようにするには、"2020/07/24 21:00:10"
のような形式にしなければなりません。

ちなみに、国際規格ISO 8601では、"2020-07-24T21:00:10"のように記述することと
定められているのですが、Microsoft Excelをはじめ、認識してくれないソフトが
あるので残念ながら今はオススメできません。そもそも世界各国で、年月日の
フォーマットが、"日/年/月"、"月/日/年"だったりとバラバラなので
それを統一するために"年-月-日"が提唱されたのですが……。

さて、話を戻して、"年/月/日 時:分:秒"にする方法を解説します。
この種の文字列置換をするには先程も登場した"fsed"コマンドが便利です。
正規表現（fsedは「拡張正規表現」に対応）を使い、次のように書けばOKです。
（参考→ http://www.kt.rim.or.jp/~kbk/regex/regex.html#POSIX にある
          「EREで使える正規表現演算子」）

以下の例は上から順に、日単位の場合、時単位の場合、分単位の場合、秒単位
の場合です。

$ fsed 's|(....)(..)(..)|\1/\2/\3|1' Tokyo2020tw_day-n.txt

$ fsed 's|(....)(..)(..)(..)|\1/\2/\3-\4:00:00|1' Tokyo2020tw_hour-n.txt

$ fsed 's|(....)(..)(..)(..)(..)|\1/\2/\3-\4:\5:00|1' Tokyo2020tw_min-n.txt

$ fsed 's|(....)(..)(..)(..)(..)(..)|\1/\2/\3-\4:\5:\6|1' Tokyo2020tw_sec-n.txt

（今は最後まで表示しても仕方無いので、途中で [CTRL]+[C]を押して止めましょう）

なおこれらの例では、秒単位の場合についても、ファイルを細切れにする前のもので
例示していますが、先程も言ったように、表計算ソフトが異常動作をしますので、
適切な範囲で細切れにしてください。（分単位、時単位の場合も加減を見ながら
細切れにすべきです）

（正規表現のことは各自調べてもらうとして、それ以外のことを）詳しく説明します。
まず、日単位ですが、この場合は"年/月/日"であればよく、"年/月/日 時:分:秒"に
する必要はありませんので、このような簡単な正規表現になっています。

一方、それ以下の粒度の場合には"年/月/日 時:分:秒"にする必要がありますが、
時単位の場合には分・秒が、分単位の場合には秒がありませんので、便宜的に"00"で
埋めています。秒単位の場合は、素直に6個の要素を当てはめています。

さて、ここで"年/月/日 時:分:秒"ではなく、"年/月/日-時:分:秒"という具合に、
年月日と時分秒の間に空白ではなく"-"（ハイフン）を置いていることに疑問を抱いた
と思います。これは、CSVファイル化するにあたって、「年月日」と「時分秒」が、
それぞれ独立した列であると誤解されることを防ぐためのトリックです。
（もちろん後で直します）

こうして、時刻フォーマットが表計算ソフトの認識可能な形式（の一歩手前）に
なったので、CSVデータに変換しましょう。変換にはTukubaiの"tocsv"という
コマンドを使えば一発です。先程と同様に、日単位、時単位、分単位、秒単位の順に
入力すべきコマンドを例示します。

$ fsed 's|(.{4})(..)(..)|\1/\2/\3|1' Tokyo2020tw_day-n.txt | tocsv 1

$ fsed 's|(.{4})(..)(..)(..)|\1/\2/\3-\4:00:00|1' Tokyo2020tw_hour-n.txt | tocsv 1

$ fsed 's|(.{4})(..)(..)(..)(..)|\1/\2/\3-\4:\5:00|1' Tokyo2020tw_min-n.txt | tocsv 1

$ fsed 's|(.{4})(..)(..)(..)(..)(..)|\1/\2/\3-\4:\5:\6|1' Tokyo2020tw_sec-n.txt | tocsv 1

（今は最後まで表示しても仕方無いので、途中で [CTRL]+[C]を押して止めましょう）

tocsvの後ろには、文字列扱いすべき列があればその列番号を入れる
（数値扱いする列番号は書かなくていい）のですが、1列目の時刻は数値でないので
この例では"1"を指定しています。

これでCSVデータに変換され、次のような内容になったはずです（秒単位の場合）。
----------
"2020/01/01-00:00:00",123
"2020/01/01-00:00:01",45
       :
       :
----------

しかし、年月日と時分秒の間を"-"で区切っていたことを思い出しましょう。
最後にこれを空白にします。fsedでやればいいことはもうわかりますね？

$ fsed 's|(.{4})(..)(..)|\1/\2/\3|1' Tokyo2020tw_day-n.txt | tocsv 1

$ fsed 's|(.{4})(..)(..)(..)|\1/\2/\3-\4:00:00|1' Tokyo2020tw_hour-n.txt | tocsv 1 | fsed 's/-/ /1'

$ fsed 's|(.{4})(..)(..)(..)(..)|\1/\2/\3-\4:\5:00|1' Tokyo2020tw_min-n.txt | tocsv 1 | fsed 's/-/ /1'

$ fsed 's|(.{4})(..)(..)(..)(..)(..)|\1/\2/\3-\4:\5:\6|1' Tokyo2020tw_sec-n.txt | tocsv 1 | fsed 's/-/ /1'

（今は最後まで表示しても仕方無いので、途中で [CTRL]+[C]を押して止めましょう）

なお、日単位のものに関してはもともと"-"が無いのでfsed不要です。
また、それ以外のものに関しても正規表現が不要なごく単純な置換のため"-e"オプションを
省略して大丈夫です。

ハイフンが消えて、中身はこんな感じなっているはずです。
----------
"2020/01/01 00:00:00",123
"2020/01/01 00:00:01",45
       :
       :
----------
それが確認できたら、最後は表計算ソフトに渡せるように、次のようにしてファイルに
書き出します。

$ fsed 's|(.{4})(..)(..)|\1/\2/\3|1' Tokyo2020tw_day-n.txt | tocsv 1 >Tokyo2020tw_day-n.csv

$ fsed 's|(.{4})(..)(..)(..)|\1/\2/\3-\4:00:00|1' Tokyo2020tw_hour-n.txt | tocsv 1 | fsed 's/-/ /1' >Tokyo2020tw_hour-n.csv

$ fsed 's|(.{4})(..)(..)(..)(..)|\1/\2/\3-\4:\5:00|1' Tokyo2020tw_min-n.txt | tocsv 1 | fsed 's/-/ /1' >Tokyo2020tw_min-n.csv

$ fsed 's|(.{4})(..)(..)(..)(..)(..)|\1/\2/\3-\4:\5:\6|1' Tokyo2020tw_sec-n.txt | tocsv 1 | fsed 's/-/ /1' >Tokyo2020tw_sec-n.csv

（拡張子部分を".csv"に変えたファイル名で保存しています）

お疲れさまでした。後は、こうしてできたCSVファイルを表計算ソフトで開いて
グラフを描き、極大点を探しながら、当時起こった出来事を推定してください。


以上
